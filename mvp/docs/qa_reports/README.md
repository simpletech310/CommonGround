# QA Reports Directory

This directory contains automated test reports generated by the Google Gemini 3 Pro QA Agent.

## Directory Structure

```
qa_reports/
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ latest.json                         # Latest session summary (quick reference)
â”œâ”€â”€ YYYY-MM-DD-HH-MM-session.md        # Full session reports
â”œâ”€â”€ failures/                           # Detailed failure analysis
â”œâ”€â”€ logs/                              # Raw test output
â”œâ”€â”€ environment-failures/              # Infrastructure issues
â”œâ”€â”€ performance/                       # Performance benchmarks
â””â”€â”€ coverage/                          # Code coverage reports
```

## Reading Reports

### Quick Status Check
```bash
# View latest test status
cat latest.json

# Check overall health
cat latest.json | grep "status"
```

### Full Session Report
```bash
# Find most recent session
ls -t *-session.md | head -1

# View the report
cat $(ls -t *-session.md | head -1)
```

### Failed Tests
```bash
# List all current failures
ls failures/

# View specific failure
cat failures/case-invitation-expiry.md
```

## Report Types

### 1. Session Reports (*.md)
Complete test run summary including:
- Executive summary
- System health status
- Module-by-module results
- New issues discovered
- Performance metrics
- Recommendations

**Frequency:** Generated after each "Make sure it works" command

### 2. latest.json
Quick JSON summary of last run for:
- CI/CD pipelines
- Status dashboards
- Programmatic checks

**Updated:** After every test session

### 3. Failure Reports (failures/*.md)
Detailed analysis of each failed test:
- Error messages
- Stack traces
- Reproduction steps
- Recommended fixes
- Impact assessment

**Created:** When new failure detected
**Updated:** When failure retested

### 4. Performance Reports (performance/*.json)
Benchmark data including:
- Response times per endpoint
- Database query performance
- Memory usage
- Trend analysis

**Frequency:** Every session

## Status Indicators

- ğŸŸ¢ **PASS/HEALTHY** - All tests passing, no issues
- ğŸŸ¡ **WARN/DEGRADED** - Some tests failing, system functional
- ğŸ”´ **FAIL/CRITICAL** - Major failures, system impaired
- âšª **SKIP/UNKNOWN** - Test not run or status unknown

## Severity Levels

- **CRITICAL** - System unusable, data loss risk, security breach
- **HIGH** - Major feature broken, blocks workflows
- **MEDIUM** - Feature impaired, workaround available
- **LOW** - Minor issue, cosmetic, edge case

## Using QA Reports in Development

### Before Starting Work
```bash
# Check if your module has known issues
cat latest.json | grep -A 5 "your_module"
```

### After Making Changes
Trigger QA agent with: **"Make sure it works"**

### Before Committing
```bash
# Verify no new failures
cat latest.json | grep "new_issues"
```

### During Code Review
Reference failure reports:
```markdown
Fixes issue #QA-042 (participant soft delete)
See: docs/qa_reports/failures/participant-removal.md
```

## Retention Policy

- **Session reports:** Keep last 30 days
- **Failure reports:** Keep until resolved + 7 days
- **Performance data:** Keep last 90 days
- **Logs:** Keep last 7 days

## Archival

Old reports archived to:
```
docs/qa_reports/archive/YYYY-MM/
```

## Emergency Contact

If QA agent reports CRITICAL issues:
1. Check `failures/` directory for details
2. Review session report for impact
3. Check `KNOWN_ISSUES.md` for workarounds
4. Address before deployment

## Integration

### CI/CD Pipeline
```yaml
# Example GitHub Action
- name: Check QA Status
  run: |
    if [ $(jq -r '.summary.failed' docs/qa_reports/latest.json) -gt 0 ]; then
      echo "âŒ QA tests failing"
      exit 1
    fi
```

### Status Dashboard
```python
# Example status check
import json

with open('docs/qa_reports/latest.json') as f:
    qa = json.load(f)

if qa['summary']['pass_rate'] < 0.95:
    print("âš ï¸ QA pass rate below 95%")
```

---

**Last Updated:** 2025-12-30
**Maintained By:** QA Automation System
**Questions?** See `docs/QA_AGENT_INSTRUCTIONS.md`
